{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Optmizacion del modelo"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# importamos las librer√≠as a utilizar\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve,roc_curve,confusion_matrix, ConfusionMatrixDisplay\n",
    "from xgboost import XGBClassifier, plot_importance"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Configuracion de las rutas para lograr las importaciones"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# obtiene la ruta absoluta del directorio 'src' desde la ubicaci√≥n del notebook\n",
    "src_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "\n",
    "# agrega la ruta a 'src' al sys.path si no est√° ya presente\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cargamos los datos pre-procesados"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df_hotel = pd.read_csv('./../data/processed/hotel_booking.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Mostramos para chequear\n",
    "df_hotel.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Mostramos informaci√≥n b√°sica\n",
    "df_hotel.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Mostramos informaci√≥n b√°sica\n",
    "df_hotel.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Columnas sobre las que trabajar y sus tipo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos las columnas de cada tipo para poder trabajar mas f√°cil"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# variable objetivo\n",
    "target: str = 'is_canceled'\n",
    "\n",
    "# columnas num√©ricas\n",
    "col_numericas: list[str] = ['lead_time', 'arrival_date_year', 'arrival_date_week_number', 'arrival_date_day_of_month', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', 'is_repeated_guest', 'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'days_in_waiting_list', 'adr', 'required_car_parking_spaces', 'total_of_special_requests']\n",
    "\n",
    "# columnas categoricas\n",
    "col_categoricas: list[str] = ['hotel', 'arrival_date_month', 'meal', 'country', 'market_segment', 'distribution_channel', 'is_repeated_guest', 'reserved_room_type', 'assigned_room_type', 'deposit_type', 'customer_type']\n",
    "\n",
    "# features\n",
    "features = col_numericas + col_categoricas\n",
    "\n",
    "# agrega a las columnas categoricas la variable objetivo\n",
    "col_categoricas = col_categoricas + [target]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Separamos los datos en entrenamiento y prueba"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from utils import split_my_data\n",
    "\n",
    "\n",
    "# Toma las variables y el target\n",
    "X = df_hotel.drop(columns=target)\n",
    "y = df_hotel[target]\n",
    "\n",
    "# divide the dataset into training and test samples\n",
    "X_train, X_test, y_train, y_test = split_my_data(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Construccion del modelo XGBoost optimizado"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inicializamos el modelo"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Inicializamos el modelo\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Definimos la estrategia de validacion"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# definir la estrategia de validaci√≥n cruzada estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Definimos las metricas e evaluacion"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# definir las m√©tricas de evaluaci√≥n\n",
    "scoring = {\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True)\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creamos la malla de hiperparametros"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# definimos la malla de hiperparametros\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500, 700],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1],\n",
    "    'scale_pos_weight': [1, 3, 5, 7]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuramos la b√∫squeda aleatoria con los parametros construidos"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,  # Ajusta el n√∫mero de iteraciones seg√∫n tu tiempo\n",
    "    scoring=scoring,\n",
    "    cv=cv,\n",
    "    refit='f1',  # M√©trica para seleccionar el mejor modelo\n",
    "    random_state=42,\n",
    "    #n_jobs=-1, Utilizar todos los n√∫cleos disponibles\n",
    "    verbose=2\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hacemos fit del modelo"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "random_search.fit(X_train, y_train)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Evaluaci√≥n del modelo"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Para ver los mejores resultados:\n",
    "print(\"Mejores hiperpar√°metros:\", random_search.best_params_)\n",
    "print(\"Mejor puntaje F1:\", random_search.best_score_)\n",
    "\n",
    "best_model = random_search.best_estimator_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predicciones y evaluaci√≥n\n",
    "y_pred = model.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "y_proba = model.predict_proba(X_test)[:, 1]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Matriz de confusion"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from draw_utils import draw_confusion_matrix\n",
    "\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nMatriz de confusion:\")\n",
    "draw_confusion_matrix(conf_matrix)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretaci√≥n r√°pida:\n",
    "\n",
    "- True Neg  **(13713)** ‚Üí Reservas no canceladas predichas correctamente.\n",
    "- True Pos  **(7237)** ‚Üí Cancelaciones correctamente detectadas.\n",
    "- False Pos **(1194)** ‚Üí Predijo cancelaci√≥n, pero la reserva no fue cancelada.\n",
    "- False Neg **(1734)** ‚Üí No predijo cancelaci√≥n, pero s√≠ se cancel√≥.\n",
    "\n",
    "Con base en estos n√∫meros, el modelo est√° haciendo un trabajo razonable, pero hay margen de mejora en los falsos negativos si nuestro objetivo es minimizar el **churn** (posibilidad de que alguien siga con el servicio)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Metrica ROC AUC"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hacemos el estudio de la metrica ROC AUC para verificar el desempeno del modelo. Se sekecciona esta metrica para el estudio debido a que la data se encuentra desbalanceada."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from draw_utils import draw_roc_auc\n",
    "\n",
    "\n",
    "draw_roc_auc(\n",
    "    y_test=y_test,\n",
    "    y_prob=y_proba,\n",
    "    g_title='Curva ROC - Cancelacion de reservacion'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seleccion del mejor threshold"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Para este valor de Auc Roc estudiamos los puntos de corte donde mejor F1-score obtengamos"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# F1 Score por cuantiles\n",
    "df_eval = pd.DataFrame({'true': y_test, 'proba': y_proba})\n",
    "# construimos los thresholds a estudiar\n",
    "thresholds = np.quantile(df_eval['proba'], np.linspace(0.1, 0.9))\n",
    "scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    # realizamos la prediccion de forma manual con base en\n",
    "    # las probabilidades que se obtienen del modelo\n",
    "    pred = (df_eval['proba'] >= t).astype(int)\n",
    "\n",
    "    #calculamos el f1-score\n",
    "    f1 = f1_score(df_eval['true'], pred)\n",
    "\n",
    "    # guardamos el score\n",
    "    scores.append((t, f1))\n",
    "\n",
    "print(\"\\nüìä --- F1 Score por punto de corte (cuantiles) ---\")\n",
    "for t, f1 in scores:\n",
    "    print(f\"Threshold: {t:.2f} | F1 Score: {f1:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# seleccionamos el mejor threshold\n",
    "best_threshold = max(scores, key=lambda x: x[1])[0]\n",
    "\n",
    "# realizamos las predicciones con este nuevo threshold de clasificacion\n",
    "y_pred_best_threshold = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n‚úÖ Mejor threshold (F1): {best_threshold:.2f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_best_threshold):.3f}\")\n",
    "print(f\"AUC (el mismo valor visto en la grafica anterior): {roc_auc_score(y_test, y_proba):.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# matriz de confusi√≥n con el nuevo threshold\n",
    "conf_matrix_with_opt_f1_score = confusion_matrix(y_test, y_pred_best_threshold)\n",
    "\n",
    "print(\"\\nMatriz de confusion:\")\n",
    "\n",
    "draw_confusion_matrix(conf_matrix_with_opt_f1_score)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Comparamos ambas matrices de confusion, la original y con el nuevo threshold"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.draw_utils import draw_comparison_confusion_matrices\n",
    "\n",
    "draw_comparison_confusion_matrices(\n",
    "    confusion_1=conf_matrix,\n",
    "    confusion_2=conf_matrix_with_opt_f1_score,\n",
    "    confusion_matrix_1_name='Modelo default',\n",
    "    confusion_matrix_2_name='Modelo con el threshold que optimiza f1-score'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Podemos obsevar que con el nuevo threshold se tiene un mejor desempeno con los falsos postivos, valores extremadamente importantes para el modelo. Por lo tanto es mejor utilizar el mdoelo con el threshold encotnrado que maximisa el f1-score."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Grafico precision - recall"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from draw_utils import draw_pr_auc\n",
    "\n",
    "\n",
    "draw_pr_auc(\n",
    "    y_test=y_test,\n",
    "    y_prob=y_proba,\n",
    "    g_title='Curva ROC - Cancelacion de reservacion'\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Podemos observar que nuestro modelo a tratar de mejorar el Recall disminuye la precision. Por lo tanto, al tratar de mejorar la capacidad del modelo de identificar las personas que cancelaran la capacida de identificar las personas que no cancelan disminuira."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Interpretacion SHAP"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import shap\n",
    "\n",
    "# interpretaci√≥n SHAP\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "La variable con mas importancia que podemos ver en este gr√°fico (deposit_type), es la que tiene mas impacto en el modelo, es decir que los depositos que no se realizaron generan mas probabilidad de cancelaci√≥n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# M√©tricas de clasificaci√≥n\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el modelo posee buenas m√©tricas con valores de precision, recall y f1-score cercanos por lo tanto parece no existir un alto overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Estudio de la importancia de cada variable"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Interpretaci√≥n del modelo ‚Äì Importancia de Variables\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Visualizaci√≥n de importancia de las caracter√≠sticas\n",
    "plot_importance(model, max_num_features=10, height=0.5)\n",
    "plt.title(\"Importancia de Variables - XGBoost\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretaci√≥n de Importancia de Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las top 3 variables que m√°s influyen en la predicci√≥n de cancelaciones son:\n",
    "\n",
    "1. lead_time (Tiempo entre reserva y llegada) ‚Üí cuanto mayor, m√°s riesgo de cancelaci√≥n.\n",
    "2. adr\t(Precio medio por noche) ‚Üí precios altos pueden ser m√°s susceptibles a cancelaci√≥n.\n",
    "3. country\t(Origen del hu√©sped) ‚Üí posiblemente refleja patrones culturales o restricciones."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Crear DataFrame con importancias\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "# Ordenar por importancia descendente\n",
    "feature_importances.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Mostrar tabla\n",
    "feature_importances.reset_index(drop=True, inplace=True)\n",
    "feature_importances.head(15)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos la gr√°fica de feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hallazgos clave de la importancia de variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable                   | Interpretaci√≥n estrat√©gica para churn |\n",
    "|----------------------------|----------------------------------------|\n",
    "| `deposit_type`             | La m√°s influyente. Si no hay dep√≥sito, el cliente puede cancelar sin penalizaci√≥n. Esto **deber√≠a revisarse** como pol√≠tica de negocio. |\n",
    "| `required_car_parking_spaces` | Clientes que requieren estacionamiento parecen m√°s comprometidos con su estad√≠a. |\n",
    "| `previous_cancellations`  | Los que han cancelado antes, tienden a hacerlo de nuevo. Perfil de cliente riesgoso. |\n",
    "| `market_segment`          | El canal de origen de la reserva afecta la tasa de cancelaci√≥n. Canales online (OTA) suelen tener m√°s cancelaciones. |\n",
    "| `total_of_special_requests` | Clientes con solicitudes especiales tienden a ser m√°s fieles. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cambio, variables como meal, distribution_channel, y assigned_room_type tienen bajo impacto predictivo en este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones del modelo de cancelaci√≥n (churn) con XGBoost\n",
    "\n",
    "**1. Variables m√°s influyentes:**\n",
    "- `deposit_type`: La pol√≠tica de dep√≥sito es el factor m√°s determinante. Las reservas sin dep√≥sito tienen alta tasa de cancelaci√≥n.\n",
    "- `required_car_parking_spaces`: Los clientes que solicitan estacionamiento parecen estar m√°s comprometidos.\n",
    "- `previous_cancellations`: El historial del cliente predice comportamiento futuro: quienes ya cancelaron, lo har√°n de nuevo.\n",
    "\n",
    "**2. Implicancias de negocio:**\n",
    "- Reforzar pol√≠ticas de dep√≥sito m√≠nimo o penalizaci√≥n en segmentos con alta cancelaci√≥n.\n",
    "- Priorizar promociones hacia segmentos con baja propensi√≥n a cancelar (p.ej., quienes hacen solicitudes especiales).\n",
    "- Evaluar y controlar canales de reserva con alto churn (p.ej., ciertos `market_segment` o `distribution_channel`).\n",
    "\n",
    "**3. Recomendaciones adicionales:**\n",
    "- Implementar alertas tempranas para reservas con alto `lead_time` y sin dep√≥sito.\n",
    "- Ofrecer beneficios adicionales a clientes frecuentes que nunca han cancelado (`previous_bookings_not_canceled` alto).\n",
    "- Reentrenar el modelo regularmente para adaptarse a cambios de comportamiento por estacionalidad o eventos externos.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ El modelo XGBoost ofrece una buena capacidad predictiva y gu√≠a acciones concretas para reducir la tasa de cancelaciones.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
